# -*- coding: utf-8 -*-
"""NewsSummarizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W-mIqAHiAuq-PDVZ3hYqulbKfcyaFSdN
"""

from os import path
import string

# Load a single document into memory
def load_doc(filename):
    try:
        with open(filename, encoding='utf-8') as file:
            text = file.read()
        return text
    except FileNotFoundError:
        print(f"File '{filename}' not found.")
        return None

# Split a document into news story and highlights
def split_story(doc):
    index = doc.find('@highlight')
    story, highlights = doc[:index], doc[index:].split('@highlight')
    highlights = [h.strip() for h in highlights if len(h) > 0]
    return story, highlights

# Load a specific story
filename = '/content/000c835555db62e319854d9f8912061cdca1893e.story'
loaded_text = load_doc(filename)
if loaded_text:
    story, highlights = split_story(loaded_text)
    cleaned_story = clean_lines(story.split('\n'))
    cleaned_highlights = clean_lines(highlights)
    print("Cleaned Story:")
    print(cleaned_story)
    print("\nCleaned Highlights:")
    print(cleaned_highlights)

from os import path
import string

# Load a single document into memory
def load_doc(filename):
    try:
        with open(filename, encoding='utf-8') as file:
            text = file.read()
        return text
    except FileNotFoundError:
        print(f"File '{filename}' not found.")
        return None

# Split a document into news story and highlights
def split_story(doc):
    index = doc.find('@highlight')
    story, highlights = doc[:index], doc[index:].split('@highlight')
    highlights = [h.strip() for h in highlights if len(h) > 0]
    return story, highlights

# Load a specific story
filename = '/content/000c835555db62e319854d9f8912061cdca1893e.story'
loaded_text = load_doc(filename)
if loaded_text:
    story, highlights = split_story(loaded_text)
    cleaned_story = clean_lines(story.split('\n'))
    cleaned_highlights = clean_lines(highlights)
    print("Cleaned Story:")
    print(cleaned_story)
    print("\nCleaned Highlights:")
    print(cleaned_highlights)

from os import path
import string

# Load a single document into memory
def load_doc(filename):
    try:
        with open(filename, encoding='utf-8') as file:
            text = file.read()
        return text
    except FileNotFoundError:
        print(f"File '{filename}' not found.")
        return None

# Split a document into news story and highlights
def split_story(doc):
    index = doc.find('@highlight')
    story, highlights = doc[:index], doc[index:].split('@highlight')
    highlights = [h.strip() for h in highlights if len(h) > 0]
    return story, highlights

# Clean lines by removing leading/trailing whitespace and empty lines
def clean_lines(lines):
    cleaned = []
    for line in lines:
        # Remove leading and trailing whitespace
        line = line.strip()
        # Remove empty lines
        if len(line) > 0:
            cleaned.append(line)
    return cleaned

# Load a specific story
filename = '/content/000c835555db62e319854d9f8912061cdca1893e.story'
loaded_text = load_doc(filename)
if loaded_text:
    story, highlights = split_story(loaded_text)
    cleaned_story = clean_lines(story.split('\n'))
    cleaned_highlights = clean_lines(highlights)
    print("Cleaned Story:")
    print(cleaned_story)
    print("\nCleaned Highlights:")
    print(cleaned_highlights)

from sklearn.feature_extraction.text import CountVectorizer

# Load a single document into memory
def load_doc(filename):
    try:
        with open(filename, encoding='utf-8') as file:
            text = file.read()
        return text
    except FileNotFoundError:
        print(f"File '{filename}' not found.")
        return None

# Split a document into story and highlights
def split_story(doc):
    index = doc.find('@highlight')
    if index == -1:
        return doc, None  # If no highlights, return story only
    story = doc[:index]
    highlights = doc[index:].split('@highlight')
    highlights = [h.strip() for h in highlights if len(h) > 0]
    return story, highlights

# Clean lines by removing leading/trailing whitespace and empty lines
def clean_lines(lines):
    cleaned = []
    for line in lines:
        line = line.strip()
        if len(line) > 0:
            cleaned.append(line)
    return cleaned

# Load and preprocess a specific story
filename = '/content/000c835555db62e319854d9f8912061cdca1893e.story'
loaded_text = load_doc(filename)

if loaded_text:
    story, _ = split_story(loaded_text)  # We don't need highlights for BoW
    cleaned_story = clean_lines(story.split('\n'))  # Clean story lines

    # Create a CountVectorizer instance
    vectorizer = CountVectorizer()

    # Fit and transform the cleaned story
    X = vectorizer.fit_transform(cleaned_story)

    # Get the feature names (unique words)
    feature_names = vectorizer.get_feature_names_out()

    # Print the Bag-of-Words matrix
    print("Bag-of-Words Matrix:")
    print(X.toarray())

    # Print feature names
    print("\nFeature Names:")
    print(feature_names)
else:
    print("Text could not be loaded.")

!pip install spacy scikit-learn
!python -m spacy download en_core_web_sm

import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# Load spaCy's English model
nlp = spacy.load('en_core_web_sm')

# Preprocess the text into sentences
def preprocess_text(text):
    doc = nlp(text)
    sentences = [sent.text.strip() for sent in doc.sents]
    return sentences

# Summarize using TF-IDF to rank important sentences
def extractive_summarization_tfidf(text, top_n=3):
    sentences = preprocess_text(text)

    # Vectorize the sentences using TF-IDF
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(sentences)

    # Compute sentence scores by summing TF-IDF values
    sentence_scores = np.array(tfidf_matrix.sum(axis=1)).ravel()

    # Rank sentences by their score and select the top_n sentences
    ranked_sentences_indices = sentence_scores.argsort()[-top_n:][::-1]

    # Create the summary by selecting the top-ranked sentences
    summary = [sentences[i] for i in ranked_sentences_indices]

    return ' '.join(summary)

# Example usage
cleaned_story = """
For the second time during his papacy, Pope Francis has announced a new group of bishops and archbishops set to become cardinals -- and they come from all over the world.
Pope Francis said Sunday that he would hold a meeting of cardinals on February 14 "during which I will name 15 new Cardinals who, coming from 13 countries from every continent, manifest the indissoluble links between the Church of Rome and the particular Churches present in the world," according to Vatican Radio.
New cardinals are always important because they set the tone in the church and also elect the next pope, CNN Senior Vatican Analyst John L. Allen said. They are sometimes referred to as the princes of the Catholic Church.
The new cardinals come from countries such as Ethiopia, New Zealand and Myanmar.
"This is a pope who very much wants to reach out to people on the margins, and you clearly see that in this set," Allen said. "You're talking about cardinals from typically overlooked places, like Cape Verde, the Pacific island of Tonga, Panama, Thailand, Uruguay."
But for the second time since Francis' election, no Americans made the list.
"Francis' pattern is very clear: He wants to go to the geographical peripheries rather than places that are already top-heavy with cardinals," Allen said.
Christopher Bellitto, a professor of church history at Kean University in New Jersey, noted that Francis announced his new slate of cardinals on the Catholic Feast of the Epiphany, which commemorates the visit of the Magi to Jesus' birthplace in Bethlehem.
"On feast of three wise men from far away, the Pope's choices for cardinal say that every local church deserves a place at the big table."
In other words, Francis wants a more decentralized church and wants to hear reform ideas from small communities that sit far from Catholicism's power centers, Bellitto said.
That doesn't mean Francis is the first pontiff to appoint cardinals from the developing world, though. Beginning in the 1920s, an increasing number of Latin American churchmen were named cardinals, and in the 1960s, St. John XXIII, whom Francis canonized last year, appointed the first cardinals from Japan, the Philippines and Africa.
In addition to the 15 new cardinals Francis named on Sunday, five retired archbishops and bishops will also be honored as cardinals.
Last year, Pope Francis appointed 19 new cardinals, including bishops from Haiti and Burkina Faso.
CNN's Daniel Burke and Christabelle Fombu contributed to this report.
"""

extractive_summary = extractive_summarization_tfidf(cleaned_story, top_n=3)
print("Extractive Summary:\n", extractive_summary)

from transformers import pipeline

def abstractive_summarization(text, model='facebook/bart-large-cnn'):
    summarizer = pipeline("summarization", model=model)
    summary = summarizer(text, max_length=150, min_length=40, do_sample=False)
    return summary[0]['summary_text']

# Example usage
cleaned_story = 'Your cleaned news article here.'
abstractive_summary = abstractive_summarization(cleaned_story)
print("Abstractive Summary:\n", abstractive_summary)

from transformers import pipeline

# Abstractive summarization using a pre-trained model
def abstractive_summarization(text, model_name='facebook/bart-large-cnn', max_chunk_length=512):
    summarizer = pipeline("summarization", model=model_name)

    # Split the text into smaller chunks if it's too long
    text_chunks = [text[i:i + max_chunk_length] for i in range(0, len(text), max_chunk_length)]

    # Summarize each chunk
    summaries = []
    for chunk in text_chunks:
        summary = summarizer(chunk, max_length=150, min_length=40, do_sample=False)
        summaries.append(summary[0]['summary_text'])

    # Combine the summaries of all chunks
    full_summary = ' '.join(summaries)

    return full_summary

# Example usage
filename = '/content/000c835555db62e319854d9f8912061cdca1893e.story'
loaded_text = load_doc(filename)

if loaded_text:
    story, _ = split_story(loaded_text)  # Use only the story part
    if story:
        abstractive_summary = abstractive_summarization(story)
        print("Abstractive Summary:\n", abstractive_summary)
    else:
        print("No story found in the document.")
else:
    print("Text could not be loaded.")

!pip install rouge_score

from rouge_score import rouge_scorer

# Function to evaluate the generated summary using ROUGE scores
def evaluate_summary(reference_summary, generated_summary):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
    scores = scorer.score(reference_summary, generated_summary)
    return scores

# Example usage
reference_summary = "Pope Francis has announced a group of new cardinals from different countries to showcase the Church's diversity."  # Human-generated reference summary
generated_summary = abstractive_summary  # Or use extractive_summary, depending on what you want to evaluate

# Evaluate and print ROUGE scores
rouge_scores = evaluate_summary(reference_summary, generated_summary)
print("ROUGE Scores:\n", rouge_scores)

